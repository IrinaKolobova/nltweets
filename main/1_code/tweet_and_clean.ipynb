{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tweepy\n",
    "import spacy\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd, numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API credentials\n",
    "credentialsPath = r'..\\0_data\\credentials'\n",
    "with open(os.path.join(credentialsPath, 'twitter_credentials.json')) as cred_data:\n",
    "    info = json.load(cred_data)\n",
    "    consumer_key = info['CONSUMER_KEY']\n",
    "    consumer_secret = info['CONSUMER_SECRET']\n",
    "    access_key = info['ACCESS_KEY']\n",
    "    access_secret = info['ACCESS_SECRET']\n",
    "\n",
    "# Create the api endpoint\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of tweets that you want to extract- 100\n",
      "Enter which you are searching for, a hashtag (#), or a mention (@)- @\n",
      "Enter the term you want to scrape- sfmta\n"
     ]
    }
   ],
   "source": [
    "# Convert to data frame and capture each column feature in an array\n",
    "def toDataFrame(tweets):\n",
    "    DataSet = pd.DataFrame()\n",
    "    \n",
    "    # Get tweet, username, and user ID\n",
    "    tweetsText = []\n",
    "    tweetsUserID = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        if 'retweeted_status' in  dir(tweet):\n",
    "            tweetsText.append(tweet.retweeted_status.full_text.encode('utf-8'))                \n",
    "        else:\n",
    "            tweetsText.append(tweet.full_text.encode('utf-8'))\n",
    "                \n",
    "        tweetsUserID.append(tweet.user.id)\n",
    "            \n",
    "    DataSet['Text'] = [text for text in tweetsText]\n",
    "    DataSet['User'] = [tweet.user.name.encode('utf-8') for tweet in tweets]\n",
    "    DataSet['UserID'] = [ID for ID in tweetsUserID]\n",
    "    \n",
    "    # Get images\n",
    "    tweetsImages = []\n",
    "    for tweet in tweets:\n",
    "        if 'media' in tweet.entities:\n",
    "            for image in tweet.entities['media']:\n",
    "                tweetsImages.append(image['media_url'])\n",
    "        else:\n",
    "            tweetsImages.append(np.nan)\n",
    "            \n",
    "    # Get Longitudes\n",
    "    DataSet['Image Urls'] = [image for image in tweetsImages]\n",
    "            \n",
    "    tweetsLongitudes = []\n",
    "    for tweet in tweets:\n",
    "        if tweet.coordinates is not None:\n",
    "            tweetsLongitudes.append(tweet.coordinates[\"coordinates\"][0])\n",
    "        else:\n",
    "            tweetsLongitudes.append(np.nan)\n",
    "            \n",
    "    DataSet['Longitude'] = [longitude for longitude in tweetsLongitudes]\n",
    "    \n",
    "    \n",
    "    # Get Latitudes\n",
    "    tweetsLatitudes = []\n",
    "    for tweet in tweets:\n",
    "        if tweet.coordinates is not None:\n",
    "            tweetsLatitudes.append(tweet.coordinates[\"coordinates\"][1])\n",
    "        else:\n",
    "            tweetsLatitudes.append(np.nan)\n",
    "            \n",
    "    DataSet['Latitude'] = [latitude for latitude in tweetsLatitudes]\n",
    "    \n",
    "    # Get created_at\n",
    "    tweetsPosted = []\n",
    "    for tweet in tweets:\n",
    "        tweetsPosted.append(tweet.created_at)\n",
    "            \n",
    "    DataSet['Created'] = [created for created in tweetsPosted]\n",
    "    \n",
    "    # Get tweet IDs\n",
    "    tweetIDs = []\n",
    "    for tweet in tweets:\n",
    "        tweetIDs.append(tweet.id)\n",
    "            \n",
    "    DataSet['tweetID'] = [ID for ID in tweetIDs]\n",
    "    \n",
    "    # Get other fields\n",
    "    locations = []\n",
    "    for tweet in tweets:\n",
    "        if tweet.user.location is not None:\n",
    "            locations.append(tweet.user.location)\n",
    "        else:\n",
    "            locations.append(np.nan)\n",
    "            \n",
    "    DataSet['location'] = [loc for loc in locations]\n",
    "   # DataSet['screen_name'] = [name for name in screen_name]\n",
    "    \n",
    "    return DataSet\n",
    "\n",
    "# Specify the maximum number of tweets that you want to be extracted.\n",
    "maximum_number_of_tweets_to_be_extracted = \\\n",
    "    int(input('Enter the number of tweets that you want to extract- '))\n",
    "\n",
    "# Specify if you are looking for hashtags or mentions\n",
    "# Note: mentions will get you exactly a mention and are case insensitive\n",
    "# Note 2: hashtags are case insensitive and not required\n",
    "# Example: datascience, Datascience, #datascience, and #Datascience will return tweets about data science\n",
    "# Hashtags of course are explicit mentions of a term, but are not case sensitive on Twitter\n",
    "tag_choice = input('Enter which you are searching for, a hashtag (#), or a mention (@)- ')\n",
    "\n",
    "# Specify the term you want to scrape\n",
    "mention = input('Enter the term you want to scrape- ')\n",
    "\n",
    "results = []\n",
    "\n",
    "for tweet_info in tweepy.Cursor(api.search, q=str(tag_choice) + mention,\n",
    "                           tweet_mode='extended').items(maximum_number_of_tweets_to_be_extracted):\n",
    "    results.append(tweet_info)\n",
    "\n",
    "data = toDataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clean Tweet</th>\n",
       "      <th>Clean Username</th>\n",
       "      <th>User ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Image URLs</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>About the only good thing about @SFMTA all nig...</td>\n",
       "      <td>Andrea Mallis</td>\n",
       "      <td>97576864</td>\n",
       "      <td>Berkeley, California</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>About the only good thing about @SFMTA all nig...</td>\n",
       "      <td>Dennis O'Donnell</td>\n",
       "      <td>191325456</td>\n",
       "      <td></td>\n",
       "      <td>http://pbs.twimg.com/ext_tw_video_thumb/110826...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@udderlydelight @SFMTA ikr!? over here in the ...</td>\n",
       "      <td>yuna yuna</td>\n",
       "      <td>238625955</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@yuna_yuna_____ @SFMTA Its ridiculous!!!!!!</td>\n",
       "      <td>japanese barbeque fingers</td>\n",
       "      <td>485270552</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@udderlydelight @SFMTA muni being sus as usual</td>\n",
       "      <td>yuna yuna</td>\n",
       "      <td>238625955</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Clean Tweet  \\\n",
       "0  About the only good thing about @SFMTA all nig...   \n",
       "1  About the only good thing about @SFMTA all nig...   \n",
       "2  @udderlydelight @SFMTA ikr!? over here in the ...   \n",
       "3        @yuna_yuna_____ @SFMTA Its ridiculous!!!!!!   \n",
       "4     @udderlydelight @SFMTA muni being sus as usual   \n",
       "\n",
       "              Clean Username    User ID              Location  \\\n",
       "0              Andrea Mallis   97576864  Berkeley, California   \n",
       "1           Dennis O'Donnell  191325456                         \n",
       "2                  yuna yuna  238625955     San Francisco, CA   \n",
       "3  japanese barbeque fingers  485270552                         \n",
       "4                  yuna yuna  238625955     San Francisco, CA   \n",
       "\n",
       "                                          Image URLs  Latitude  Longitude  \n",
       "0                                                NaN       NaN        NaN  \n",
       "1  http://pbs.twimg.com/ext_tw_video_thumb/110826...       NaN        NaN  \n",
       "2                                                NaN       NaN        NaN  \n",
       "3                                                NaN       NaN        NaN  \n",
       "4                                                NaN       NaN        NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the clean text and user arrays\n",
    "clean_texts = []\n",
    "clean_users = []\n",
    "\n",
    "# Clean the text data\n",
    "for row in data.itertuples():\n",
    "    text = row[1].decode('utf-8').encode('ascii', 'ignore').decode('ascii').strip()\n",
    "    user = row[2].decode('utf-8').encode('ascii', 'ignore').decode('ascii').strip()\n",
    "    \n",
    "#     text_doc = nlp.make_doc(text)\n",
    "#     tokens = [token for token in text_doc if not token.is_stop]\n",
    "#     tokens = [token for token in tokens if token.text != ' ']    \n",
    "#     tokenz_final = [token.text for token in tokens]\n",
    "    \n",
    "#     text = \" \".join(tokenz_final).strip()\n",
    "    text = text.replace('\\n', '')\n",
    "    \n",
    "    clean_texts.append(text)\n",
    "    clean_users.append(user)\n",
    "    \n",
    "clean_df = pd.DataFrame({'Clean Tweet':clean_texts, 'Clean Username':clean_users,\n",
    "                         'User ID':data['UserID'],\n",
    "                         'Location':data['location'],\n",
    "                         'Image URLs':data['Image Urls'],\n",
    "                         'Latitude':data['Latitude'],\n",
    "                         'Longitude':data['Longitude']})\n",
    "\n",
    "# Remove the usernames (and thus tweets) that use unicode (were coerced to empty strings by the previous step)\n",
    "# This means there are no non-English tweets/usernames\n",
    "indexNames = clean_df[(clean_df['Clean Username'] == '') | (clean_df['Clean Username'] == '@')].index\n",
    "\n",
    "clean_df.drop(indexNames, inplace=True)\n",
    "\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = r'..\\0_data\\manual'\n",
    "filePath = os.path.join(outputPath,'tweets_with_mention_' + mention + '.csv')\n",
    "if not os.path.isfile(filePath):\n",
    "    clean_df.to_csv(filePath, index=False)\n",
    "else:\n",
    "    with open(filePath, 'a') as file:\n",
    "        clean_df.to_csv(file, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
