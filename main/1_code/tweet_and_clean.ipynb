{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(45000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 45 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 45\n",
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tweepy\n",
    "import spacy\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API credentials\n",
    "credentialsPath = r'..\\0_data\\credentials'\n",
    "with open(os.path.join(credentialsPath, 'twitter_credentials.json')) as cred_data:\n",
    "    info = json.load(cred_data)\n",
    "    consumer_key = info['CONSUMER_KEY']\n",
    "    consumer_secret = info['CONSUMER_SECRET']\n",
    "    access_key = info['ACCESS_KEY']\n",
    "    access_secret = info['ACCESS_SECRET']\n",
    "\n",
    "# Create the api endpoint\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of tweets that you want to extract- 40\n",
      "Enter which you are searching for, a hashtag (#), or a mention (@)- #\n",
      "Enter the term you want to scrape- lol\n",
      "Extracted 40 tweets with #lol\n"
     ]
    }
   ],
   "source": [
    "# Convert to data frame and capture each column feature in an array\n",
    "def toDataFrame(tweets):\n",
    "    DataSet = pd.DataFrame()\n",
    "    \n",
    "    tweetsText = []\n",
    "    for tweet in tweets:\n",
    "            if 'retweeted_status' in  dir(tweet):\n",
    "                tweetsText.append(tweet.retweeted_status.full_text.encode('utf-8'))                \n",
    "            else:\n",
    "                tweetsText.append(tweet.full_text.encode('utf-8'))\n",
    "            \n",
    "    DataSet['Text'] = [text for text in tweetsText]\n",
    "    DataSet['User'] = [tweet.user.name.encode('utf-8') for tweet in tweets]\n",
    "    \n",
    "    tweetsImages = []\n",
    "    for tweet in tweets:\n",
    "        if 'media' in tweet.entities:\n",
    "            for image in tweet.entities['media']:\n",
    "                tweetsImages.append(image['media_url'])\n",
    "        else:\n",
    "            tweetsImages.append('')\n",
    "            \n",
    "    \n",
    "    DataSet['Image Urls'] = [image for image in tweetsImages]\n",
    "            \n",
    "    tweetsLongitudes = []\n",
    "    for tweet in tweets:\n",
    "        if tweet.coordinates is not None:\n",
    "            tweetsLongitudes.append(tweet.coordinates[\"coordinates\"][0])\n",
    "        else:\n",
    "            tweetsLongitudes.append('')\n",
    "    DataSet['Longitude'] = [longitude for longitude in tweetsLongitudes]\n",
    "    \n",
    "    tweetsLatitudes = []\n",
    "    for tweet in tweets:\n",
    "        if tweet.coordinates is not None:\n",
    "            tweetsLatitudes.append(tweet.coordinates[\"coordinates\"][1])\n",
    "        else:\n",
    "            tweetsLatitudes.append('')\n",
    "    DataSet['Latitude'] = [latitude for latitude in tweetsLatitudes]\n",
    "    \n",
    "    return DataSet\n",
    "\n",
    "\n",
    "# Specify the maximum number of tweets that you want to be extracted.\n",
    "maximum_number_of_tweets_to_be_extracted = \\\n",
    "    int(input('Enter the number of tweets that you want to extract- '))\n",
    "\n",
    "# Specify if you are looking for hashtags or mentions\n",
    "# Note: mentions will get you exactly a mention and are case insensitive\n",
    "# Note 2: hashtags are case insensitive and not required\n",
    "# Example: datascience, Datascience, #datascience, and #Datascience will return tweets about data science\n",
    "# Hashtags of course are explicit mentions of a term, but are not case sensitive on Twitter\n",
    "tag_choice = input('Enter which you are searching for, a hashtag (#), or a mention (@)- ')\n",
    "\n",
    "# Specify the term you want to scrape\n",
    "mention = input('Enter the term you want to scrape- ')\n",
    "\n",
    "results = []\n",
    "\n",
    "for tweet_info in tweepy.Cursor(api.search, q=str(tag_choice) + mention,\n",
    "                           tweet_mode='extended').items(maximum_number_of_tweets_to_be_extracted):\n",
    "    results.append(tweet_info)\n",
    "    \n",
    "for tweet_info in tweepy.Cursor(api.search, q=str(tag_choice) + mention.upper(),\n",
    "                           tweet_mode='extended').items(maximum_number_of_tweets_to_be_extracted):\n",
    "    results.append(tweet_info)\n",
    "\n",
    "data = toDataFrame(results)\n",
    "outputPath = r'..\\0_data\\manual'\n",
    "filePath = os.path.join(outputPath,'tweets_with_mention_' + mention + '.csv')\n",
    "if not os.path.isfile(filePath):\n",
    "    data.to_csv(filePath, index=False)\n",
    "else:\n",
    "    with open(filePath, 'a') as file:\n",
    "        data.to_csv(file, index = False)\n",
    "print ('Extracted ' + str(maximum_number_of_tweets_to_be_extracted) \\\n",
    "    + ' tweets with ' + str(tag_choice) + mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.Tagger at 0x119cccf0>),\n",
       " ('parser', <spacy.pipeline.DependencyParser at 0xfa04ed0>),\n",
       " ('ner', <spacy.pipeline.EntityRecognizer at 0xfa91ed0>)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the spacy library and English corpus\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clean text and user arrays\n",
    "clean_texts = []\n",
    "clean_users = []\n",
    "\n",
    "# Clean the text data\n",
    "for row in data.itertuples():\n",
    "    text = row[1].decode('utf-8').encode('ascii', 'ignore').decode('ascii').strip()\n",
    "    user = row[2].decode('utf-8').encode('ascii', 'ignore').decode('ascii').strip()\n",
    "    \n",
    "#     text_doc = nlp.make_doc(text)\n",
    "#     tokens = [token for token in text_doc if not token.is_stop]\n",
    "#     tokens = [token for token in tokens if token.text != ' ']    \n",
    "#     tokenz_final = [token.text for token in tokens]\n",
    "    \n",
    "#     text = \" \".join(tokenz_final).strip()\n",

    "    text = text.replace('\\n', '')\n",
    "    \n",
    "    clean_texts.append(text)\n",
    "    clean_users.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.DataFrame({'Clean Tweet':clean_texts, 'Clean Username':clean_users})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clean Tweet</th>\n",
       "      <th>Clean Username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#scubadiving #scuba #diving #diver #scubadiver...</td>\n",
       "      <td>Dive Bot 2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There Are More Fish In The Sea https://t.co/nk...</td>\n",
       "      <td>Tami Quigley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My kind of #fish https://t.co/OV7kR2jj7H</td>\n",
       "      <td>Erick Scarecrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Get some grilling on your plate! The inhouse r...</td>\n",
       "      <td>Chateau Garli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lionfish ... perfect #art for the #fishing #en...</td>\n",
       "      <td>Lois Bryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Give a man a #fish and he will eat for a day. ...</td>\n",
       "      <td>CharlestonBoozeChef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What #crudo dreams are made of... #OpenBlue #C...</td>\n",
       "      <td>GoodGREENSF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Give a man a #fish and he will eat for a day. ...</td>\n",
       "      <td>Fishing Skills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wow great meal today salad and fish/shrimp and...</td>\n",
       "      <td>Adam [TeEm] [ICN] [FAMafia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sauteed Tuna by THE HUSBAND  #WINEsdish #foode...</td>\n",
       "      <td>Desiree Charmaine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What makes a fishes life any less important th...</td>\n",
       "      <td>Compton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>We have a shirt for every fisherman at CheekyB...</td>\n",
       "      <td>Kristie C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>#scubadiving #scuba #diving #diver #scubadiver...</td>\n",
       "      <td>Buceo - Scuba News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://t.co/GPg2jMPyQD Hit the link subscribe...</td>\n",
       "      <td>AnglerHubRT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>50% Off Bottle Labels, Towels, Cosmetics Bags,...</td>\n",
       "      <td>Gertrude Quackerz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>There Are More Fish In The Sea https://t.co/ET...</td>\n",
       "      <td>Tami Quigley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>La tarde!!!#vivelaplaya #friends #traveling #t...</td>\n",
       "      <td>Chiringuito Beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Trailscapes ... Fine Art Photography by Tami Q...</td>\n",
       "      <td>Tami Quigley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>#scubadiving #scuba #diving #diver #scubadiver...</td>\n",
       "      <td>Amine Lounes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>#scubadiving #scuba #diving #diver #scubadiver...</td>\n",
       "      <td>Dive Bot 2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>There Are More Fish In The Sea https://t.co/nk...</td>\n",
       "      <td>Tami Quigley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>My kind of #fish https://t.co/OV7kR2jj7H</td>\n",
       "      <td>Erick Scarecrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Get some grilling on your plate! The inhouse r...</td>\n",
       "      <td>Chateau Garli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Lionfish ... perfect #art for the #fishing #en...</td>\n",
       "      <td>Lois Bryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Give a man a #fish and he will eat for a day. ...</td>\n",
       "      <td>CharlestonBoozeChef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What #crudo dreams are made of... #OpenBlue #C...</td>\n",
       "      <td>GoodGREENSF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Give a man a #fish and he will eat for a day. ...</td>\n",
       "      <td>Fishing Skills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Wow great meal today salad and fish/shrimp and...</td>\n",
       "      <td>Adam [TeEm] [ICN] [FAMafia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Sauteed Tuna by THE HUSBAND  #WINEsdish #foode...</td>\n",
       "      <td>Desiree Charmaine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>What makes a fishes life any less important th...</td>\n",
       "      <td>Compton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>We have a shirt for every fisherman at CheekyB...</td>\n",
       "      <td>Kristie C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>#scubadiving #scuba #diving #diver #scubadiver...</td>\n",
       "      <td>Buceo - Scuba News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>https://t.co/GPg2jMPyQD Hit the link subscribe...</td>\n",
       "      <td>AnglerHubRT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>50% Off Bottle Labels, Towels, Cosmetics Bags,...</td>\n",
       "      <td>Gertrude Quackerz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>There Are More Fish In The Sea https://t.co/ET...</td>\n",
       "      <td>Tami Quigley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>La tarde!!!#vivelaplaya #friends #traveling #t...</td>\n",
       "      <td>Chiringuito Beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Trailscapes ... Fine Art Photography by Tami Q...</td>\n",
       "      <td>Tami Quigley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>#scubadiving #scuba #diving #diver #scubadiver...</td>\n",
       "      <td>Amine Lounes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Clean Tweet  \\\n",
       "0   #scubadiving #scuba #diving #diver #scubadiver...   \n",
       "1   There Are More Fish In The Sea https://t.co/nk...   \n",
       "2            My kind of #fish https://t.co/OV7kR2jj7H   \n",
       "3   Get some grilling on your plate! The inhouse r...   \n",
       "4   Lionfish ... perfect #art for the #fishing #en...   \n",
       "5   Give a man a #fish and he will eat for a day. ...   \n",
       "6   What #crudo dreams are made of... #OpenBlue #C...   \n",
       "7   Give a man a #fish and he will eat for a day. ...   \n",
       "8   Wow great meal today salad and fish/shrimp and...   \n",
       "9   Sauteed Tuna by THE HUSBAND  #WINEsdish #foode...   \n",
       "10  What makes a fishes life any less important th...   \n",
       "12  We have a shirt for every fisherman at CheekyB...   \n",
       "13  #scubadiving #scuba #diving #diver #scubadiver...   \n",
       "16  https://t.co/GPg2jMPyQD Hit the link subscribe...   \n",
       "17  50% Off Bottle Labels, Towels, Cosmetics Bags,...   \n",
       "19  There Are More Fish In The Sea https://t.co/ET...   \n",
       "21  La tarde!!!#vivelaplaya #friends #traveling #t...   \n",
       "23  Trailscapes ... Fine Art Photography by Tami Q...   \n",
       "24  #scubadiving #scuba #diving #diver #scubadiver...   \n",
       "25  #scubadiving #scuba #diving #diver #scubadiver...   \n",
       "26  There Are More Fish In The Sea https://t.co/nk...   \n",
       "27           My kind of #fish https://t.co/OV7kR2jj7H   \n",
       "28  Get some grilling on your plate! The inhouse r...   \n",
       "29  Lionfish ... perfect #art for the #fishing #en...   \n",
       "30  Give a man a #fish and he will eat for a day. ...   \n",
       "31  What #crudo dreams are made of... #OpenBlue #C...   \n",
       "32  Give a man a #fish and he will eat for a day. ...   \n",
       "33  Wow great meal today salad and fish/shrimp and...   \n",
       "34  Sauteed Tuna by THE HUSBAND  #WINEsdish #foode...   \n",
       "35  What makes a fishes life any less important th...   \n",
       "37  We have a shirt for every fisherman at CheekyB...   \n",
       "38  #scubadiving #scuba #diving #diver #scubadiver...   \n",
       "41  https://t.co/GPg2jMPyQD Hit the link subscribe...   \n",
       "42  50% Off Bottle Labels, Towels, Cosmetics Bags,...   \n",
       "44  There Are More Fish In The Sea https://t.co/ET...   \n",
       "46  La tarde!!!#vivelaplaya #friends #traveling #t...   \n",
       "48  Trailscapes ... Fine Art Photography by Tami Q...   \n",
       "49  #scubadiving #scuba #diving #diver #scubadiver...   \n",
       "\n",
       "                 Clean Username  \n",
       "0                  Dive Bot 2.0  \n",
       "1                  Tami Quigley  \n",
       "2               Erick Scarecrow  \n",
       "3                 Chateau Garli  \n",
       "4                    Lois Bryan  \n",
       "5           CharlestonBoozeChef  \n",
       "6                   GoodGREENSF  \n",
       "7                Fishing Skills  \n",
       "8   Adam [TeEm] [ICN] [FAMafia]  \n",
       "9             Desiree Charmaine  \n",
       "10                      Compton  \n",
       "12                    Kristie C  \n",
       "13           Buceo - Scuba News  \n",
       "16                  AnglerHubRT  \n",
       "17            Gertrude Quackerz  \n",
       "19                 Tami Quigley  \n",
       "21            Chiringuito Beach  \n",
       "23                 Tami Quigley  \n",
       "24                 Amine Lounes  \n",
       "25                 Dive Bot 2.0  \n",
       "26                 Tami Quigley  \n",
       "27              Erick Scarecrow  \n",
       "28                Chateau Garli  \n",
       "29                   Lois Bryan  \n",
       "30          CharlestonBoozeChef  \n",
       "31                  GoodGREENSF  \n",
       "32               Fishing Skills  \n",
       "33  Adam [TeEm] [ICN] [FAMafia]  \n",
       "34            Desiree Charmaine  \n",
       "35                      Compton  \n",
       "37                    Kristie C  \n",
       "38           Buceo - Scuba News  \n",
       "41                  AnglerHubRT  \n",
       "42            Gertrude Quackerz  \n",
       "44                 Tami Quigley  \n",
       "46            Chiringuito Beach  \n",
       "48                 Tami Quigley  \n",
       "49                 Amine Lounes  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the usernames (and thus tweets) that use unicode (were coerced to empty strings by the previous step)\n",
    "# This means there are no non-English tweets/usernames\n",
    "indexNames = clean_df[(clean_df['Clean Username'] == '') | (clean_df['Clean Username'] == '@')].index\n",
    "\n",
    "clean_df.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = r'..\\0_data\\manual'\n",
    "filePath = os.path.join(outputPath,'tweets_with_mention_' + mention + '.csv')\n",
    "if not os.path.isfile(filePath):\n",
    "    clean_df.to_csv(filePath, index=False)\n",
    "else:\n",
    "    with open(filePath, 'a') as file:\n",
    "        clean_df.to_csv(file, index = False)"

   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
