{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(45000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 45 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 45\n",
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tweepy\n",
    "import spacy\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API credentials\n",
    "credentialsPath = r'..\\0_data\\credentials'\n",
    "with open(os.path.join(credentialsPath, 'twitter_credentials.json')) as cred_data:\n",
    "    info = json.load(cred_data)\n",
    "    consumer_key = info['CONSUMER_KEY']\n",
    "    consumer_secret = info['CONSUMER_SECRET']\n",
    "    access_key = info['ACCESS_KEY']\n",
    "    access_secret = info['ACCESS_SECRET']\n",
    "\n",
    "# Create the api endpoint\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of tweets that you want to extract- 10000\n",
      "Enter which you are searching for, a hashtag (#), or a mention (@)- @\n",
      "Enter the term you want to scrape- twitter\n"
     ]
    },
    {
     "ename": "TweepError",
     "evalue": "Twitter error response: status code = 429",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTweepError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-6e8c6c794f26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m for tweet_info in tweepy.Cursor(api.search, q=str(tag_choice) + mention,\n\u001b[1;32m---> 62\u001b[1;33m                            tweet_mode='extended').items(maximum_number_of_tweets_to_be_extracted):\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\riley\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\riley\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[1;31m# Reached end of current page, get the next page...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\riley\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__self__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\riley\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;31m# Set pagination mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\riley\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;31m# Parse the response payload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTweepError\u001b[0m: Twitter error response: status code = 429"
     ]
    }
   ],
   "source": [
    "# Convert to data frame and capture each column feature in an array\n",
    "def toDataFrame(tweets):\n",
    "    DataSet = pd.DataFrame()\n",
    "    \n",
    "    tweetsText = []\n",
    "    for tweet in tweets:\n",
    "            if 'retweeted_status' in  dir(tweet):\n",
    "                tweetsText.append(tweet.retweeted_status.full_text.encode('utf-8'))                \n",
    "            else:\n",
    "                tweetsText.append(tweet.full_text.encode('utf-8'))\n",
    "            \n",
    "    DataSet['Text'] = [text for text in tweetsText]\n",
    "    DataSet['User'] = [tweet.user.name.encode('utf-8') for tweet in tweets]\n",
    "    \n",
    "    tweetsImages = []\n",
    "    for tweet in tweets:\n",
    "        if 'media' in tweet.entities:\n",
    "            for image in tweet.entities['media']:\n",
    "                tweetsImages.append(image['media_url'])\n",
    "        else:\n",
    "            tweetsImages.append('')\n",
    "            \n",
    "    \n",
    "    DataSet['Image Urls'] = [image for image in tweetsImages]\n",
    "            \n",
    "    tweetsLongitudes = []\n",
    "    for tweet in tweets:\n",
    "        if tweet.coordinates is not None:\n",
    "            tweetsLongitudes.append(tweet.coordinates[\"coordinates\"][0])\n",
    "        else:\n",
    "            tweetsLongitudes.append('')\n",
    "    DataSet['Longitude'] = [longitude for longitude in tweetsLongitudes]\n",
    "    \n",
    "    tweetsLatitudes = []\n",
    "    for tweet in tweets:\n",
    "        if tweet.coordinates is not None:\n",
    "            tweetsLatitudes.append(tweet.coordinates[\"coordinates\"][1])\n",
    "        else:\n",
    "            tweetsLatitudes.append('')\n",
    "    DataSet['Latitude'] = [latitude for latitude in tweetsLatitudes]\n",
    "    \n",
    "    return DataSet\n",
    "\n",
    "\n",
    "# Specify the maximum number of tweets that you want to be extracted.\n",
    "maximum_number_of_tweets_to_be_extracted = \\\n",
    "    int(input('Enter the number of tweets that you want to extract- '))\n",
    "\n",
    "# Specify if you are looking for hashtags or mentions\n",
    "# Note: mentions will get you exactly a mention and are case insensitive\n",
    "# Note 2: hashtags are case insensitive and not required\n",
    "# Example: datascience, Datascience, #datascience, and #Datascience will return tweets about data science\n",
    "# Hashtags of course are explicit mentions of a term, but are not case sensitive on Twitter\n",
    "tag_choice = input('Enter which you are searching for, a hashtag (#), or a mention (@)- ')\n",
    "\n",
    "# Specify the term you want to scrape\n",
    "mention = input('Enter the term you want to scrape- ')\n",
    "\n",
    "results = []\n",
    "\n",
    "for tweet_info in tweepy.Cursor(api.search, q=str(tag_choice) + mention,\n",
    "                           tweet_mode='extended').items(maximum_number_of_tweets_to_be_extracted):\n",
    "    results.append(tweet_info)\n",
    "    \n",
    "for tweet_info in tweepy.Cursor(api.search, q=str(tag_choice) + mention.upper(),\n",
    "                           tweet_mode='extended').items(maximum_number_of_tweets_to_be_extracted):\n",
    "    results.append(tweet_info)\n",
    "\n",
    "data = toDataFrame(results)\n",
    "outputPath = r'..\\0_data\\manual'\n",
    "filePath = os.path.join(outputPath,'tweets_with_mention_' + mention + '.csv')\n",
    "if not os.path.isfile(filePath):\n",
    "    data.to_csv(filePath, index=False)\n",
    "else:\n",
    "    with open(filePath, 'a') as file:\n",
    "        data.to_csv(file, index = False)\n",
    "print ('Extracted ' + str(maximum_number_of_tweets_to_be_extracted) \\\n",
    "    + ' tweets with ' + str(tag_choice) + mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clean text and user arrays\n",
    "clean_texts = []\n",
    "clean_users = []\n",
    "\n",
    "# Clean the text data\n",
    "for row in data.itertuples():\n",
    "    text = row[1].decode('utf-8').encode('ascii', 'ignore').decode('ascii').strip()\n",
    "    user = row[2].decode('utf-8').encode('ascii', 'ignore').decode('ascii').strip()\n",
    "    \n",
    "#     text_doc = nlp.make_doc(text)\n",
    "#     tokens = [token for token in text_doc if not token.is_stop]\n",
    "#     tokens = [token for token in tokens if token.text != ' ']    \n",
    "#     tokenz_final = [token.text for token in tokens]\n",
    "    \n",
    "#     text = \" \".join(tokenz_final).strip()\n",
    "    text = text.replace('\\n', '')\n",
    "    \n",
    "    clean_texts.append(text)\n",
    "    clean_users.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.DataFrame({'Clean Tweet':clean_texts, 'Clean Username':clean_users})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clean Tweet</th>\n",
       "      <th>Clean Username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#scubadiving #scuba #diving #diver #scubadiver...</td>\n",
       "      <td>Dive Bot 2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There Are More Fish In The Sea https://t.co/nk...</td>\n",
       "      <td>Tami Quigley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My kind of #fish https://t.co/OV7kR2jj7H</td>\n",
       "      <td>Erick Scarecrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Get some grilling on your plate! The inhouse r...</td>\n",
       "      <td>Chateau Garli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lionfish ... perfect #art for the #fishing #en...</td>\n",
       "      <td>Lois Bryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Give a man a #fish and he will eat for a day. ...</td>\n",
       "      <td>CharlestonBoozeChef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What #crudo dreams are made of... #OpenBlue #C...</td>\n",
       "      <td>GoodGREENSF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Give a man a #fish and he will eat for a day. ...</td>\n",
       "      <td>Fishing Skills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wow great meal today salad and fish/shrimp and...</td>\n",
       "      <td>Adam [TeEm] [ICN] [FAMafia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sauteed Tuna by THE HUSBAND  #WINEsdish #foode...</td>\n",
       "      <td>Desiree Charmaine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What makes a fishes life any less important th...</td>\n",
       "      <td>Compton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>We have a shirt for every fisherman at CheekyB...</td>\n",
       "      <td>Kristie C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>#scubadiving #scuba #diving #diver #scubadiver...</td>\n",
       "      <td>Buceo - Scuba News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://t.co/GPg2jMPyQD Hit the link subscribe...</td>\n",
       "      <td>AnglerHubRT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>50% Off Bottle Labels, Towels, Cosmetics Bags,...</td>\n",
       "      <td>Gertrude Quackerz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>There Are More Fish In The Sea https://t.co/ET...</td>\n",
       "      <td>Tami Quigley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>La tarde!!!#vivelaplaya #friends #traveling #t...</td>\n",
       "      <td>Chiringuito Beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Trailscapes ... Fine Art Photography by Tami Q...</td>\n",
       "      <td>Tami Quigley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>#scubadiving #scuba #diving #diver #scubadiver...</td>\n",
       "      <td>Amine Lounes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>#scubadiving #scuba #diving #diver #scubadiver...</td>\n",
       "      <td>Dive Bot 2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>There Are More Fish In The Sea https://t.co/nk...</td>\n",
       "      <td>Tami Quigley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>My kind of #fish https://t.co/OV7kR2jj7H</td>\n",
       "      <td>Erick Scarecrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Get some grilling on your plate! The inhouse r...</td>\n",
       "      <td>Chateau Garli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Lionfish ... perfect #art for the #fishing #en...</td>\n",
       "      <td>Lois Bryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Give a man a #fish and he will eat for a day. ...</td>\n",
       "      <td>CharlestonBoozeChef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What #crudo dreams are made of... #OpenBlue #C...</td>\n",
       "      <td>GoodGREENSF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Give a man a #fish and he will eat for a day. ...</td>\n",
       "      <td>Fishing Skills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Wow great meal today salad and fish/shrimp and...</td>\n",
       "      <td>Adam [TeEm] [ICN] [FAMafia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Sauteed Tuna by THE HUSBAND  #WINEsdish #foode...</td>\n",
       "      <td>Desiree Charmaine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>What makes a fishes life any less important th...</td>\n",
       "      <td>Compton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>We have a shirt for every fisherman at CheekyB...</td>\n",
       "      <td>Kristie C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>#scubadiving #scuba #diving #diver #scubadiver...</td>\n",
       "      <td>Buceo - Scuba News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>https://t.co/GPg2jMPyQD Hit the link subscribe...</td>\n",
       "      <td>AnglerHubRT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>50% Off Bottle Labels, Towels, Cosmetics Bags,...</td>\n",
       "      <td>Gertrude Quackerz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>There Are More Fish In The Sea https://t.co/ET...</td>\n",
       "      <td>Tami Quigley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>La tarde!!!#vivelaplaya #friends #traveling #t...</td>\n",
       "      <td>Chiringuito Beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Trailscapes ... Fine Art Photography by Tami Q...</td>\n",
       "      <td>Tami Quigley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>#scubadiving #scuba #diving #diver #scubadiver...</td>\n",
       "      <td>Amine Lounes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Clean Tweet  \\\n",
       "0   #scubadiving #scuba #diving #diver #scubadiver...   \n",
       "1   There Are More Fish In The Sea https://t.co/nk...   \n",
       "2            My kind of #fish https://t.co/OV7kR2jj7H   \n",
       "3   Get some grilling on your plate! The inhouse r...   \n",
       "4   Lionfish ... perfect #art for the #fishing #en...   \n",
       "5   Give a man a #fish and he will eat for a day. ...   \n",
       "6   What #crudo dreams are made of... #OpenBlue #C...   \n",
       "7   Give a man a #fish and he will eat for a day. ...   \n",
       "8   Wow great meal today salad and fish/shrimp and...   \n",
       "9   Sauteed Tuna by THE HUSBAND  #WINEsdish #foode...   \n",
       "10  What makes a fishes life any less important th...   \n",
       "12  We have a shirt for every fisherman at CheekyB...   \n",
       "13  #scubadiving #scuba #diving #diver #scubadiver...   \n",
       "16  https://t.co/GPg2jMPyQD Hit the link subscribe...   \n",
       "17  50% Off Bottle Labels, Towels, Cosmetics Bags,...   \n",
       "19  There Are More Fish In The Sea https://t.co/ET...   \n",
       "21  La tarde!!!#vivelaplaya #friends #traveling #t...   \n",
       "23  Trailscapes ... Fine Art Photography by Tami Q...   \n",
       "24  #scubadiving #scuba #diving #diver #scubadiver...   \n",
       "25  #scubadiving #scuba #diving #diver #scubadiver...   \n",
       "26  There Are More Fish In The Sea https://t.co/nk...   \n",
       "27           My kind of #fish https://t.co/OV7kR2jj7H   \n",
       "28  Get some grilling on your plate! The inhouse r...   \n",
       "29  Lionfish ... perfect #art for the #fishing #en...   \n",
       "30  Give a man a #fish and he will eat for a day. ...   \n",
       "31  What #crudo dreams are made of... #OpenBlue #C...   \n",
       "32  Give a man a #fish and he will eat for a day. ...   \n",
       "33  Wow great meal today salad and fish/shrimp and...   \n",
       "34  Sauteed Tuna by THE HUSBAND  #WINEsdish #foode...   \n",
       "35  What makes a fishes life any less important th...   \n",
       "37  We have a shirt for every fisherman at CheekyB...   \n",
       "38  #scubadiving #scuba #diving #diver #scubadiver...   \n",
       "41  https://t.co/GPg2jMPyQD Hit the link subscribe...   \n",
       "42  50% Off Bottle Labels, Towels, Cosmetics Bags,...   \n",
       "44  There Are More Fish In The Sea https://t.co/ET...   \n",
       "46  La tarde!!!#vivelaplaya #friends #traveling #t...   \n",
       "48  Trailscapes ... Fine Art Photography by Tami Q...   \n",
       "49  #scubadiving #scuba #diving #diver #scubadiver...   \n",
       "\n",
       "                 Clean Username  \n",
       "0                  Dive Bot 2.0  \n",
       "1                  Tami Quigley  \n",
       "2               Erick Scarecrow  \n",
       "3                 Chateau Garli  \n",
       "4                    Lois Bryan  \n",
       "5           CharlestonBoozeChef  \n",
       "6                   GoodGREENSF  \n",
       "7                Fishing Skills  \n",
       "8   Adam [TeEm] [ICN] [FAMafia]  \n",
       "9             Desiree Charmaine  \n",
       "10                      Compton  \n",
       "12                    Kristie C  \n",
       "13           Buceo - Scuba News  \n",
       "16                  AnglerHubRT  \n",
       "17            Gertrude Quackerz  \n",
       "19                 Tami Quigley  \n",
       "21            Chiringuito Beach  \n",
       "23                 Tami Quigley  \n",
       "24                 Amine Lounes  \n",
       "25                 Dive Bot 2.0  \n",
       "26                 Tami Quigley  \n",
       "27              Erick Scarecrow  \n",
       "28                Chateau Garli  \n",
       "29                   Lois Bryan  \n",
       "30          CharlestonBoozeChef  \n",
       "31                  GoodGREENSF  \n",
       "32               Fishing Skills  \n",
       "33  Adam [TeEm] [ICN] [FAMafia]  \n",
       "34            Desiree Charmaine  \n",
       "35                      Compton  \n",
       "37                    Kristie C  \n",
       "38           Buceo - Scuba News  \n",
       "41                  AnglerHubRT  \n",
       "42            Gertrude Quackerz  \n",
       "44                 Tami Quigley  \n",
       "46            Chiringuito Beach  \n",
       "48                 Tami Quigley  \n",
       "49                 Amine Lounes  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the usernames (and thus tweets) that use unicode (were coerced to empty strings by the previous step)\n",
    "# This means there are no non-English tweets/usernames\n",
    "indexNames = clean_df[(clean_df['Clean Username'] == '') | (clean_df['Clean Username'] == '@')].index\n",
    "\n",
    "clean_df.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = r'..\\0_data\\manual'\n",
    "filePath = os.path.join(outputPath,'tweets_with_mention_' + mention + '.csv')\n",
    "if not os.path.isfile(filePath):\n",
    "    clean_df.to_csv(filePath, index=False)\n",
    "else:\n",
    "    with open(filePath, 'a') as file:\n",
    "        clean_df.to_csv(file, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
